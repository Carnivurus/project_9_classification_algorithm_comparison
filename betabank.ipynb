{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "Beta Bank customers are leaving, ony by one, every month. Bankers discovered that it is cheaper to save existing customers than to attract new ones. `We need to predict whether a customer will leave the bank soon.` The data about the past behavior of clients and the termination of contracts with the bank will be provided.\n",
    "\n",
    "## Objective\n",
    "Create a model with the maximum possible F1 value to pass this test, the value should be at least 0.59. Additionally, I will measure the AUC-ROC metric to compare the F1 values\n",
    "\n",
    "## Way to work\n",
    "Along this project I focused on training and obtaining the best possible metrics for the models. You will find next the following structure:\n",
    "\n",
    "- Preparing the data (Importing libraries and datasets).\n",
    "- Preprocessing the data (Verifying duplicates, errors in datasets, null values, maintaining relevant information etc).\n",
    "- Training a model (analyze the parameters to be improve and necessary process techniques).\n",
    "- Rebalancing the datasets (oversampling)\n",
    "- Training 3 classification models (Random Forest, SVM and Linear Regression)\n",
    "- Conclusions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "### Importing libraries\n",
    "\n",
    "These are the libraries that will be used along the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Tripleten/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "In order to provide a complete analysis, first I need to ensure the imported dataset is prepared to be used for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quick view of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing dtypes and null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking duplicated rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous code I identified the following statements:\n",
    "- name columns are OK\n",
    "- dtypes are classified correctly \n",
    "- no duplicates found\n",
    "- `Tenure has null values.`\n",
    "- `\"Geography\" and \"Gender\" needs to be encoded into binary codes.`\n",
    "- `Will be necessary to apply standard scaler`\n",
    "\n",
    "Also, in my consideration; \"RowNumber\", \"CustomerID\" and \"Surname\" are not relevant for the model training, I'll proceed removing them and do the necessary changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['RowNumber','CustomerId', 'Surname'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple code to analyze the % of null values\n",
    "((df['Tenure'].isna().sum())/df.shape[0] )*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will focus on the null values for Tenure, I'm considering two options:\n",
    "\n",
    "- Eliminate the rows which represents 9.09% of the dataset (909 null values of the total 10000)\n",
    "- Look for the median of the column Tenure and replace the null values.\n",
    "\n",
    "I will proceed with the first option, avoiding to integrate fictitious data in our dataset. However the main objective is to predict the users who will leave the bank (value 1 of column 'Exited'), so, before removing the data I will analyze the impact of deleting the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the shape of the df\n",
    "print(f'Total Rows: {df.shape[0]}')\n",
    "\n",
    "# Analyzing null rows for the Tenure column\n",
    "print(f'Total Null Rows: {df['Tenure'].isnull().sum()}',  end='\\n\\n')\n",
    "\n",
    "# Distribution of the exited values.\n",
    "exited_null_rows = df['Exited'].value_counts()\n",
    "print(f'Excited values distribution: \\n{exited_null_rows}',end='\\n\\n')\n",
    "\n",
    "# Distribution of the exited results for tenure null values. \n",
    "tenure_exited_null_rows = df[df['Tenure'].isnull()]['Exited'].value_counts()\n",
    "print(f'Excited values distribution for null rows: \\n{tenure_exited_null_rows}', end='\\n\\n')\n",
    "\n",
    "print(f'The percentage of missing 1 values to be eliminated will be {((tenure_exited_null_rows[1]*100)/exited_null_rows[1]):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 10000, from which 909 are null rows. Inside those 909 null rows we have 183 null tenure values. Considering the percentage of the values that will be eliminated we can proceed to delete it (does not represent high looses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # To revalidate all null values came from Tenure column\n",
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the changes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] # 9091 values\n",
    "df.shape[0] + 909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming into 'integer' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tenure']= df['Tenure'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping to remove any data, I will plot the data to see if something is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight data seems right, means and medians are pretty similar for all the columns, standard deviation show a high variability for 'Balance' and 'Estimated Salary' columns. \n",
    "\n",
    "In the other hand a box plot chart could help to identify atypical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns =df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Creating a 3.3 figure that plots all the integer and float types.\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(num_columns):\n",
    "    row = i // 3\n",
    "    col = i % 3    \n",
    "    axs[row, col].boxplot(df[column])\n",
    "    axs[row, col].set_title(column)\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the previous charts we can infer the following:\n",
    "\n",
    "Credit score: We have atypical values (outliers in the lower whisker), could be several situations like new clients with a credit history.\n",
    "\n",
    "- Age: The analysis shows that the majority of the clients are concentrated between 20 - 60 years, however we can find users above this age.\n",
    "- Tenure: Seems stable with a median of 5 and a max value of 10\n",
    "- Balance: shows a highly concentration of the values located in the lower part, while some specific values represent high values\n",
    "- Num of products: shows the value 1 as the median, however some clients can have up to 4 products.\n",
    "- Has credit card and Is Active Member: Have a median of 1 having all their values in 0 or 1\n",
    "- Estimated Salary: Have a normal distribution, not showing atypical values.\n",
    "- Existe: Shows that the distribuition of clients remains with the banc, having atypical situation with users who left the banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a figure that plot CreditScore and Age histograms\n",
    "# fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "num_columns =df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Creating a 3.3 figure that plots all the integer and float types.\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(num_columns):\n",
    "    row = i // 3\n",
    "    col = i % 3    \n",
    "    axs[row, col].hist(df[column])\n",
    "    axs[row, col].set_title(f'{column} distribution')\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram graphs reinforce the highlights commented before.Let's also analize the categoric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a figure that plot the object types\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.hist(df['Geography'], color=['green'])\n",
    "ax1.set_title('Geography distribution')\n",
    "ax2.hist(df['Gender'])\n",
    "ax2.set_title('Gender distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the clients from france represents the half market of the product, while the distribution between males and females is similar. In the following charts we will filter the Geography and Gender analysis for users who left the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit_users_df = df[df['Exited']==1]\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.hist(quit_users_df['Geography'], color='green')\n",
    "ax1.set_title('Geography (user who left the bank)')\n",
    "ax2.hist(quit_users_df['Gender'])\n",
    "ax2.set_title('Gender (user who left the bank)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are interesting, Germany and France are the countries where users left the bank account. Now that we now more about the columns let's analyze their correlation. The one hot encoding will be important to classified the categoric columns into binaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding\n",
    "\n",
    "The dataset has been preproceed, let's focus on apply the OHE in our dataframe, remember get_dummies method will only affect the objects columns, transforming into binary bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = pd.get_dummies(data=df, dummy_na=False )\n",
    "data_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for the correlation between our column exited and the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data_ohe.corr()\n",
    "corr_with_exited = data_ohe.corr()['Exited'].sort_values(ascending=False)\n",
    "\n",
    "print(corr_with_exited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the analyze of the correlation between the column 'Exited' and the other columns, we could not highlight big visible references, however thr age and the geography shows a little positive correlation. If we compare the correlation matrix, we can see the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "Now that our dataset is prepared, it will need to be split into two datasets to train to test our model. To do this, I'm setting `'X'` (features) and `'y'` (objective) to use it as parameters for the method train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_ohe.drop(columns='Exited')\n",
    "y = data_ohe['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.25,  random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use the class `GridSearchCV` which will allow to grid my search when training my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'n_estimators': np.arange(1,101,10),\n",
    "'max_depth': np.arange(1,6,1) ,\n",
    "}\n",
    "\n",
    "scoring = {'accuracy': 'accuracy', \n",
    "           'recall': 'recall',\n",
    "           'f1': 'f1'}\n",
    "\n",
    "rfc_gr = GridSearchCV(RandomForestClassifier(random_state=1000), param_grid=params, cv=3, verbose=2 ,scoring =scoring, refit='f1')\n",
    "rfc_gr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to eliminate the `precision ` parameter in the previous training model because it was sending a warning explaining that the model could not predict the class due to an imbalance. In other words it is necessary `to assign weight to the class 1 for the parameter 'class_weight'`, or `apply class rebalancing techniques like oversampling or undersampling`.\n",
    "\n",
    "Before starting with the rebalancing, I will check the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_balance_scores_df = pd.DataFrame(rfc_gr.cv_results_)\n",
    "best_row = rfc_gr.best_index_\n",
    "no_balance_results = no_balance_scores_df.iloc[best_row,:]\n",
    "cols = ['mean_test_f1', 'mean_test_accuracy', 'mean_test_recall', 'params' ]\n",
    "\n",
    "print(f'Best results of the test')\n",
    "no_balance_results[cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test shows an harmonic mean ('f1') of 42.51% which is not enough for our objective. I will continue with the prediction to se if the results can vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc_gr.best_estimator_.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_results = pd.DataFrame(report).transpose()\n",
    "print(df_results ,end='\\n\\n')\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of our model is to predict whether a customer will leave the bank soon (cancelling the account). We need to focus on the classification of `1`, the current model has a highly ratio of precision and recall for the classification `0` (users who will not leave the bank), however recall for the element `1` is just 34%, that means the following:\n",
    "\n",
    "From the total elements in our dataset, the model could only identify 34.5% of elements classified as `1` while its precision of them it reach the 80%, giving us an harmonic mean (f1 score) of 48.27%\n",
    "\n",
    "This is not enough for our goal, I'll focus on rebalancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir en una tabla similar el porcentaje de usuarios que permanece y el % de personas que no permanece en funcion de las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_table = pd.DataFrame(y_pred, columns=['prediction'])\n",
    "leaving_users = predict_table[predict_table['prediction']== 1]['prediction'].count()\n",
    "staying_users = predict_table[predict_table['prediction']== 0]['prediction'].count()\n",
    "\n",
    "y_test_table = pd.DataFrame(data=y_test, columns=['prediction'])\n",
    "r_leaving_users = y_test_table[y_test_table['prediction']== 1]['prediction'].count()\n",
    "r_staying_users = y_test_table[y_test_table['prediction']== 0]['prediction'].count()\n",
    "\n",
    "\n",
    "y_test\n",
    "np.array(y_tes)\n",
    "# per_leaving_users = (100*leaving_users/(leaving_users+staying_users))\n",
    "# per_staying_users = (100 - per_leaving_users)\n",
    "\n",
    "# r_per_leaving_users = (100*r_leaving_users/(r_leaving_users+r_staying_users))\n",
    "# r_per_staying_users = (100 - r_per_leaving_users)\n",
    "\n",
    "\n",
    "# print(f'Total users {predict_table.value_counts()}', end='\\n\\n')\n",
    "# # print(f'Percentage of leaving users: {per_leaving_users:.2f}%')\n",
    "\n",
    "\n",
    "# fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "# ax1.bar(x=[f'Leaving users ({per_leaving_users:.2f}%)', f'Staying users ({per_staying_users:.2f}%)'], height=[leaving_users, staying_users])\n",
    "# ax1.set_title('Predicted user behaviour')\n",
    "\n",
    "# ax2.bar(x=[f'Leaving users ({r_per_leaving_users:.2f}%)', f'Staying users ({r_per_staying_users:.2f}%)'], height=[r_leaving_users, r_staying_users])\n",
    "# ax2.set_title('Predicted user behaviour')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "According to the prediction of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite class_weight is a good option for rebalancing, I will explore SVM algorithms which are not capable of receive this parameters, as a solution I will use oversampling.\n",
    "\n",
    "I will start leaving the sampling_strategy by default ('auto') which will balance the binaries classifications into 50% each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "print(y_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = y_resampled.value_counts()[0]\n",
    "var2 = y_resampled.value_counts()[1]\n",
    "\n",
    "plt.bar(x=['0','1'], height=[var1,var2], )\n",
    "plt.title('Exited Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected the dataset is now balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training new models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will train 3 classification algorithm models to find a better result. I will use GridsearchCV to wrap all of them. The classification models will be.\n",
    "\n",
    "- Random Forest Classifier. \n",
    "- Support Vector Machine.\n",
    "- Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Pipelines\n",
    "\n",
    "To keep a consistent result I will lock the random_state into 1000 for the creation of each model, in the specific case of SVC I will change the probability value for True (otherwise it will give an error because probabilities are being used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(random_state=1000))])\n",
    "pipe_svc = Pipeline([('scaler', StandardScaler()), ('svc', SVC(random_state=1000, probability=True))])\n",
    "pipe_lr = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(random_state=1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I nested the parameters into dictionaries as a solution provided by GridSearchCV classifications, I'll also apply different evaluation metrics to go in detail with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\n",
    "    'rf__n_estimators': np.arange(1, 100, 10),\n",
    "    'rf__max_depth': np.arange(1, 6, 1)},\n",
    "    {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel':['rbf'], \n",
    "    'svc__gamma':['scale'],\n",
    "    'svc__degree':np.arange(1, 4, 1)},\n",
    "    {\n",
    "    'lr__penalty':['elasticnet'],\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "    'lr__solver': ['saga'],\n",
    "    'lr__l1_ratio': [0.5],\n",
    "    'lr__max_iter': [1000]}\n",
    "]\n",
    "\n",
    "scores =['accuracy','precision', 'recall', 'f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code will englobe all my process to select the best model.\n",
    "\n",
    "Using GridSearch I iterate the 3 models for doing the following workflow:\n",
    "- Scaling data : Using standard scaler on the pipeline of each model.\n",
    "- Training resampled datasets: by adding each model the necessary parameters and looking for the best f1 score\n",
    "- Obtaining the best parameters: The trained model will obtain the best scores and will be printed\n",
    "- Testing validation datasets: Once the models are trained, It will predict the test dataset \n",
    "- Plotting ROC results: To give a better insight of the results the iteration will plot the ROC curve with the AUROC for all the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# If you cannot see syntax highlight colors is due tue the %%time\n",
    "\n",
    "# Creating a list of the models nested in a pipeline\n",
    "pipes = [pipe_rf, pipe_svc, pipe_lr]\n",
    "\n",
    "# Defining the columns we want to analyze once the model has been trained\n",
    "cols = ['mean_test_f1', 'mean_test_precision' ,'mean_test_accuracy', 'mean_test_recall']\n",
    "\n",
    "# Creating a iteration for each model with its own parameters\n",
    "for pipe, grid in zip(pipes, params):\n",
    "    gs =GridSearchCV(pipe, param_grid=grid, scoring=scores, refit='f1', cv=2)\n",
    "    gs.fit(X_resampled,y_resampled)\n",
    "\n",
    "    # The results of each model will be printed; F1 Score, Precision, Accuracy, Recall and best parameters.\n",
    "    print('_______________________________________________')\n",
    "    print(f'For the pipeline {pipe[1]}')\n",
    "    results_df = pd.DataFrame(gs.cv_results_)\n",
    "    results_df = results_df[cols]\n",
    "    best_row = gs.best_index_\n",
    "    results = results_df.iloc[best_row,:]\n",
    "    print(f'With the parameters: {gs.best_params_}', end='\\n\\n')\n",
    "    print('Te best mean test results are:')\n",
    "    print(results, end= '\\n\\n')\n",
    "\n",
    "\n",
    "    # This block is intended to show the performance for the validation process by the binaries results ('0' and '1').\n",
    "    y_pred = gs.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    print('And for the validation test we have the following results.')\n",
    "    print(report_df ,end='\\n\\n')\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test,y_pred) ,end='\\n\\n')\n",
    "\n",
    "    # This block obtain the roc_curve and auc\n",
    "    y_pred_proba = gs.predict_proba(X_test)[:,1]    \n",
    "    fpr,tpr, tresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # This block will plot a AUROC chart by each model created.\n",
    "    plt.plot(fpr, tpr, label= f'{pipe.steps[-1][0]} (AUROC) = {roc_auc:.4f}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the best F1 score for each model during the test stage wre:\n",
    " \n",
    " | Model | F1 Mean |\n",
    " |---------------------|---------------------|\n",
    " |Random Forest|0.818391|\n",
    " |SVC|0.844246|\n",
    " | Linear regression| 0.798570|\n",
    "       \n",
    "At first sight SVC seems to be the best model when comparing the f1 mean scores, however the classification report can give us deeper details for each binary classification \n",
    " \n",
    " | Model  | Classification | precision|   recall|  f1-score| support|\n",
    " |--------------|---|----------|---------|----------|-------------|\n",
    " | Random Forest| 1 |  0.544762| 0.613734|  0.577195|   466.000000|\n",
    " | SVM          | 1 |  0.623529| 0.568670|  0.594837|   466.000000|\n",
    " | Linear Regression | 1 |  0.532258| 0.424893|  0.472554|   466.000000|\n",
    "\n",
    " The results for Random Forest and SVM are close, while Random Forest is giving us a better recall, SVM is focused on getting more precision. If we focus con F1 score, the SVM should be the best model for the present options, however in a last exhaustive test, the AUROC method was applied.\n",
    "\n",
    " Giving us the following results\n",
    " \n",
    " | Model  | AUROC | \n",
    " |--------------|---|\n",
    " | Random Forest| 0.8288 | \n",
    " | SVM          | 0.8283 |  \n",
    " | Linear Regression |0.7553| \n",
    "\n",
    "\n",
    " In this case, the Random Forest method show a better slightly performance. In conclusion I will personally choose the Random Forest method because the results will get more precision when trying to predict if a user is going to leave the bank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Along this project we face different situations, the original data set was preprocessed to be able to train our models. The codification model applied was the One hot encoder (OHE) from the pandas library (get_dummies method).\n",
    "\n",
    "First, I started training one Random Forest Model to find the best f1 score, however it was necessary to balance and standardize the data to reach better results, I made those changes but including 3 new different models. The chosen classification algorithms were Random Forest, SVM, and Linear Regression, each one with different parameters to feed and create robust results. \n",
    "\n",
    "In general, It can be infer that Random Forest and SVM models are much better compared with the Linear regression, the evaluation metrics showed better results when analyzing the metrics accuracy, precision, recall and f1 score.\n",
    "\n",
    "A classification report was ran to go deeper into the analysis showing results by binaries classification (focusing the efforts in classification '1'). Finally a ROC Curve chart was plot to check the performance of each model, showing the AUC results.\n",
    "\n",
    "In conclusion the Results for Random Forest and SVM were pretty similar, but I found the Random Forest a better option for this project, because we are focusing on the precision of predicting the user who are going to leave the bank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
